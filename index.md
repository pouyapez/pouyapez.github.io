---
layout: default
---
<div class="menu-container noselect">
   <table class="content-table">
      <tr>
        <td>
          <img class="left-align image noselect" src="/images/face.png" style="width:100%">
        </td>
        <td>
          <p class="text right-align text-large add-top-margin" style="width:100%;">
             I am a PhD student in Machine Learning at the University of California Irvine, advised by Prof.                       <a href="http://sameersingh.org">Sameer Singh</a>. My main area of interest are Knowledge Graph Completion, Interpretability, Active Learning and Text Generation. 
          </p>
        </td>
      </tr>
   </table>
</div>

# Research Internships

* * *
<ul>
  <li>Semantic Machines at Microsoft Research, summer 2021.</li>
  <li>Siri Knowledge group at Apple, summer 2020.</li>
  <li>Allen Institute for Artificial Intelligence, summer 2019.</li>
  <li>Fujitsu Laboratories of America, summer 2018. </li>
  <li>Chinese University of Hong Kong, summer 2014.  </li>
</ul>

# Professional Experience
* * *

<ul>
   <li>Co-organised Explainable Graph-Based Machine Learning workshop at AKBC <a href="https://xgml.github.io">2021</a>.</li>
  <li>Co-organised Knowledge Bases and Multiple Modalities workshop at AKBC <a href="https://kb-mm.github.io">2019</a> and <a href="https://kb-mm-2020.github.io">2020</a>.</li>
   <li>2021: Reviewer at NeurIPS, NAACL</li>
  <li>2020: Reviewer at NeurIPS, ICLR, AAAI, EMNLP</li>
  <li>2019: Reviewer at NeurIPS, ICLR, EMNLP</li>
  <li>2018: Reviewer at EMNLP</li>
  <li>Volunteer at NeurIPS 2018 and AKBC 2020.</li>
</ul>


# Conference & Journal Publications

* * *

<div class="menu-container noselect">
   <table class="content-table">
    <col width="500px" />
    <col width="800px" />
      <tr>
        <td>
          <img class="left-align image noselect" src="/images/big.png" width="1000">
        </td>
        <td>
          <p class="text cright-align text-large add-top-margin" style="width:100%;" align="center">
              <font style="font-size:20px">Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models</font><br>
              <font style="font-size:15px">Aarohi Srivastava, et al.<br>
              In Submission</font><br> 
             </p><p align="justify"> <font style="font-size:13px">In this paper we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, with diverse topics about drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI‚Äôs GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. Findings include: model performance and calibration both improve with scale; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit ‚Äúbreakthrough‚Äù behavior at a critical scale often involve multiple steps; social bias typically increases with scale in ambiguous settings, but this can be improved with prompting.
 </font><br></p>
                      <p align="center">
             <font style="font-size:15px"><a href="https://arxiv.org/pdf/2206.04615.pdf">PDF</a></font>
             </p>
        </td>
      </tr>
   </table>
</div>

* * *

<div class="menu-container noselect">
   <table class="content-table">
    <col width="500px" />
    <col width="800px" />
      <tr>
        <td>
          <img class="left-align image noselect" src="/images/gain.png" width="1000">
        </td>
        <td>
          <p class="text cright-align text-large add-top-margin" style="width:100%;" align="center">
              <font style="font-size:20px">The Extremal GDoF Gain of Optimal versus Binary Power Control in ùêæ User Interference Networks Is Œò(&radic;<span style="text-decoration: overline">k</span>)</font><br>
              <font style="font-size:15px">Yao-Chia Chan, <b>Pouya Pezeshkpour</b>, Chunhua Geng, Syed A. Jafar<br>
              IEEE Transactions on Wireless Communications 2022</font><br> 
             </p><p align="justify"> <font style="font-size:13px">In this paper we explicitly characterizes the extremal GDoF gain of optimal over binary power control as Œò(&radic;<span style="text-decoration: overline">k</span>) for all ùêæ. In particular, the extremal gain is bounded between
&radic;<span style="text-decoration: overline">k</span> and 2.5 &radic;<span style="text-decoration: overline">k</span> for every ùêæ. For ùêæ = 2, 3, 4, 5, 6 users, the precise extremal gain is 1, 3/2, 2, 9/4 and 41/16, respectively. Networks shown to achieve the extremal gain may be interpreted as multi-tier heterogeneous networks. It is worthwhile to note that because of their focus on asymptotic analysis, the sharp characterizations of extremal gains are valuable primarily from a theoretical perspective, and not as contradictions to the conventional w. </font><br></p>
                      <p align="center">
             <font style="font-size:15px"><a href="https://arxiv.org/pdf/2205.02216.pdf">PDF</a></font>
             </p>
        </td>
      </tr>
   </table>
</div>

* * *


<div class="menu-container noselect">
   <table class="content-table">
    <col width="500px" />
    <col width="800px" />
      <tr>
        <td>
          <img class="left-align image noselect" src="/images/artifact.png" width="1000">
        </td>
        <td>
          <p class="text cright-align text-large add-top-margin" style="width:100%;" align="center">
              <font style="font-size:20px">Combining Feature and Instance Attribution to Detect Artifacts</font><br>
              <font style="font-size:15px"><b>Pouya Pezeshkpour</b>, Sarthak Jain, Byron Wallace, Sameer Singh<br>
              ACL 2022</font><br> 
             </p><p align="justify"> <font style="font-size:13px">In this paper we evaluate use of different attribution methods for aiding identification of training data artifacts. We propose new hybrid approaches that combine saliency maps (which highlight "important" input features) with instance attribution methods (which retrieve training samples "influential" to a given prediction). We show that this proposed training-feature attribution can be used to efficiently uncover artifacts in training data when a challenging validation set is available. We also carry out a small user study to evaluate whether these methods are useful to NLP researchers in practice, with promising results. </font><br></p>
                      <p align="center">
             <font style="font-size:15px"><a href="https://arxiv.org/pdf/2107.00323.pdf">PDF</a>&nbsp;&nbsp;&nbsp;<a href="https://www.youtube.com/watch?v=1xXnio8AdpY&t=141s">Video on Youtube</a></font>
             </p>
        </td>
      </tr>
   </table>
</div>

* * *

<div class="menu-container noselect">
   <table class="content-table">
    <col width="500px" />
    <col width="800px" />
      <tr>
        <td>
          <img class="left-align image noselect" src="/images/emp-inf.png" width="1000">
        </td>
        <td>
          <p class="text cright-align text-large add-top-margin" style="width:100%;" align="center">
              <font style="font-size:20px">An Empirical Comparison of Instance Attribution Methods for NLP</font><br>
              <font style="font-size:15px"><b>Pouya Pezeshkpour</b>, Sarthak Jain, Byron Wallace, Sameer Singh<br>
              NAACL 2021</font><br> 
             </p><p align="justify"> <font style="font-size:13px">In this work we evaluate the degree to which different potential instance attribution agree with respect to the importance of training samples.
We find that simple retrieval methods yield training instances that differ from those identified via gradient-based methods (such as the IF), but that nonetheless exhibit desirable characteristics similar to more complex attribution methods. </font><br></p>
                      <p align="center">
             <font style="font-size:15px"><a href="https://arxiv.org/pdf/2104.04128.pdf">PDF</a>&nbsp;&nbsp;&nbsp;  <a href="https://github.com/successar/instance_attributions_NLP">Source Code</a> &nbsp;&nbsp;&nbsp;   <a href="https://www.youtube.com/watch?v=b9AbcEDmFyg&t=17s">Video on Youtube</a></font>
             </p>
        </td>
      </tr>
   </table>
</div>

* * *
<div class="menu-container noselect">
   <table class="content-table">
    <col width="500px" />
    <col width="800px" />
      <tr>
        <td>
          <img class="left-align image noselect" src="/images/parsinlu.png" width="1000">
        </td>
        <td>
          <p class="text cright-align text-large add-top-margin" style="width:100%;" align="center">
              <font style="font-size:20px">PARSINLU: A Suite of Language Understanding Challenges for Persian</font><br>
              <font style="font-size:15px">Daniel Khashabi, Arman Cohan, Siamak Shakeri, Pedram Hosseini, <b>Pouya Pezeshkpour</b>, et al<br>
              TACL 2021</font><br> 
             </p><p align="justify"> <font style="font-size:13px">We introduce PARSINLU, the first benchmark in Persian language that includes a range of high-level tasks, Reading Comprehension, Textual Entailment, etc. These datasets are collected in a multitude of ways, often involving manual annotations by native speakers. This results in over 14.5k new instances across 6 distinct NLU tasks. Besides, we present the first results on state-of-the-art monolingual and multi- lingual pre-trained language models on this benchmark and compare them with human performance, which provides valuable in- sights into our ability to tackle natural language understanding challenges in Persian. </font><br></p>
                      <p align="center">
             <font style="font-size:15px"><a href="https://arxiv.org/pdf/2012.06154.pdf">PDF</a>&nbsp;&nbsp;&nbsp;  <a href="https://github.com/persiannlp/parsinlu">Source Code</a></font>
             </p>
        </td>
      </tr>
   </table>
</div>

* * * 




<div class="menu-container noselect">
   <table class="content-table">
    <col width="500px" />
    <col width="800px" />
      <tr>
        <td>
          <img class="left-align image noselect" src="/images/crowd.jpg" width="1000">
        </td>
        <td>
          <p class="text cright-align text-large add-top-margin" style="width:100%;" align="center">
              <font style="font-size:20px">Revisiting Evaluation of Knowledge Base Completion Models </font><br>
              <font style="font-size:15px"><b>Pouya Pezeshkpour</b>, Yifan Tian, Sameer Singh<br>
              <font style="color:red;">(nominated for best paper award) </font>AKBC 2020 </font><br> 
             </p><p align="justify"> <font style="font-size:13px">In this paper, we first study the shortcomings of the evaluation metrics in knowldge graph Embeddings.  More specifically, we demonstrate that these metrics 1) are unreliable for estimating calibration, 2) make strong assumptions that    are often violated, and 3) do not sufficiently, and consistently, differentiate embedding methods from simple approaches and from each other. To address these issues, we provide a semi-complete KG using a randomly sampled subgraph from the test and validation data of YAGO3-10, allowing us to compute accurate triple classification accuracy on this data. </font><br></p>
           <p align="center">
             <font style="font-size:15px"><a href="https://pouyapez.github.io/yago3-tc/">Project Page</a>  &nbsp;&nbsp;&nbsp; <a href="https://openreview.net/pdf?id=1uufzxsxfL">PDF</a>&nbsp;&nbsp;&nbsp;  <a href="https://github.com/pouyapez/yago3-tc">Source Code</a>  &nbsp;&nbsp;&nbsp;  <a href="https://www.youtube.com/watch?v=y9BHvpqHo1g&t=9s">Video on Youtube</a></font>
             </p>
        </td>
      </tr>
   </table>
</div>

* * *

<div class="menu-container noselect">
   <table class="content-table">
    <col width="500px" />
    <col width="800px" />
      <tr>
        <td>
          <img class="left-align image noselect" src="/images/criage.png" width="1000">
        </td>
        <td>
          <p class="text right-align text-large add-top-margin" style="width:100%;" align="center">
            <font style="font-size:20px">Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications </font><br>
            <font style="font-size:15px"><b>Pouya Pezeshkpour</b>, Yifan Tian, Sameer Singh<br>
            NAACL 2019</font><br>
            </p><p align="justify"><font style="font-size:13px">In this paper, we propose adversarial modifications for link prediction models: identifying the fact to add into or remove from the knowledge graph that changes the prediction for a target fact after the model is retrained. We introduce an efficient approach to estimate the effect of such modifications by approximating the change in the embeddings when the knowledge graph changes. We use these techniques to evaluate the robustness of link prediction models (by measuring sensitivity to additional facts), study interpretability through the facts most responsible for predictions (by identifying the most influential neighbors), and detect incorrect facts in the knowledge base.</font><br></p>
           <p align="center">
<font style="font-size:15px"><a href="https://pouyapez.github.io/criage/">Project Page</a>  &nbsp;&nbsp;&nbsp;  <a href="https://arxiv.org/pdf/1905.00563.pdf">PDF</a>  &nbsp;&nbsp;&nbsp;  <a href="https://github.com/pouyapez/criage">Source Code</a>  &nbsp;&nbsp;&nbsp;  <a href="https://www.youtube.com/watch?v=irVqAjt664s">Video on Youtube</a></font>
          </p>
        </td>
      </tr>
   </table>
</div>

* * *
<div class="menu-container noselect">
   <table class="content-table">
    <col width="500px" />
    <col width="800px" />
      <tr>
        <td>
          <img class="left-align image noselect" src="/images/graph.jpg" width="1000">
        </td>
        <td>
          <p class="text right-align text-large add-top-margin" align="center">
            <font style="font-size:20px">Embedding Multimodal Relational Data for Knowledge Base Completion</font><br>
            <font style="font-size:15px"><b>Pouya Pezeshkpour</b>, Liyan Chen, Sameer Singh<br>
            EMNLP 2018</font><br>
            </p><p align="justify"><font style="font-size:13px">In this work, we propose multimodal knowledge base embeddings (MKBE) that use different neural encoders for this variety of observed data, and combine them with existing relational models to learn embeddings of the entities and multimodal data. Further, using these learned embedings and different neural decoders, we introduce a novel multimodal imputation model to generate missing multimodal values, like text and images, from information in the knowledge base.</font><br></p>
           <p align="center">
<font style="font-size:15px"><a href="https://pouyapez.github.io/mkbe/">Project Page</a>  &nbsp;&nbsp;&nbsp;   <a href="https://arxiv.org/pdf/1809.01341.pdf">PDF</a>  &nbsp;&nbsp;&nbsp;   <a href="https://github.com/pouyapez/mkbe">Source Code</a>  &nbsp;&nbsp;&nbsp;   <a href="https://www.youtube.com/watch?v=Bt5CccdXHUM&t=2s">Video on Youtube</a></font>
          </p>
        </td>
      </tr>
   </table>
</div>

* * *
<div class="menu-container noselect">
   <table class="content-table">
    <col width="500px" />
    <col width="800px" />
      <tr>
        <td>
          <img class="left-align image noselect" src="/images/2014.png"  width="1000">
        </td>
        <td>
          <p class="text right-align text-large add-top-margin" align="center">
            <font style="font-size:20px">Optimal tradeoff between source and state distortions over a Gaussian channel using single and hybrid digital analog codes</font><br>
            <font style="font-size:15px"><b>Pouya Pezeshkpour</b>, Hamid Behroozi<br>
            IST'2014</font><br>
            </p><p align="justify"><font style="font-size:13px">In this paper, the problem of transmitting an analog Gaussian source over an additive white Gaussian noise (AWGN) channel in the presence of a Gaussian interference known only at the transmitter is investigated. Our goal is to estimate both the analog source and the channel state at the receiver simultaneously. In this work, we present different transmission schemes based on joint source-channel coding. We study hybrid digital-analog (HDA) joint source-channel coding schemes and analyze the region of (mean-squared error) distortion pairs (in estimating the source and the state) that are simultaneously achievable.</font><br></p>
           <p align="center">
<font style="font-size:15px"><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7000779">PDF</a></font>
          </p>
        </td>
      </tr>
   </table>
</div>

# Workshop & Symposia 

* * *

<ul>
   <li>Preethi Seshadri, <b>Pouya Pezeshkpour</b>, Sameer Singh,"<a href="https://arxiv.org/abs/2210.04337">Quantifying Social Biases Using Templates is Unreliable</a>". The TSRML workshop at NeurIPS 2022.</li>
   <li><b>Pouya Pezeshkpour</b>, Zhengli Zhao, Sameer Singh,"<a href="https://openreview.net/pdf?id=p3m_WpN0rEX">On the Utility of Active Instance Selection for Few-Shot Learning</a>". The HAMLETS workshop at NeurIPS 2020.</li>
  <li><b>Pouya Pezeshkpour</b>, Zhengli Zhao, Sameer Singh,"<a href="https://github.com/EliSchwartz/VL3-Workshop/blob/master/pdfs/36.pdf">Using Data Importance for Effective Active Learning</a>". The CVPR workshop on Visual Learning with Limited Labels (VL3), 2020.</li>
  <li><b>Pouya Pezeshkpour</b>, Yifan Tian, Sameer Singh, "Integrating Local Structure into Knowledge Graph Embeddings". SoCal NLP Symposium 2019.</li>
  <li><b>Pouya Pezeshkpour</b>, Ramya Malursrinivasan, Ajey Chander, "<a href="https://pouyapez.github.io/Generating%20User-friendly%20Explanations%20for%20Loan%20Denials%20using%20GANs.pdf">Generating User-friendly Explanations for Loan Denials using GANs</a>". NIPS 2018 Workshop on Challenges and Opportunities for AI in Financial Services.</li>
  <li><b>Pouya Pezeshkpour</b>, Carlos Guestrin, Sameer Singh, "<a href="https://arxiv.org/pdf/1805.00184.pdf">Compact Factorization of Matrices Using Generalized Round-Rank</a>". Southern California Machine Learning Symposium 2017.</li>
</ul>

# Patents
* * *

<ul>
  <li><b>Pouya Pezeshkpour</b>, Ramya Malursrinivasan, Ajay Chander, "<a href="https://patents.justia.com/patent/20200125640">USER-FRIENDLY EXPLANATION PRODUCTION USING GENERATIVE ADVERSARIAL NETWORKS</a>". US Patent Number 20200125640, 2020.</li>
 <li><b>Pouya Pezeshkpour</b>, Ramya Malursrinivasan, Ajey Chander, "<a href="https://patents.justia.com/patent/20200125975">EXPLANATIONS GENERATION WITH DIFFERENT COGNITIVE VALUES USING GENERATIVE ADVERSARIAL NETWORKS</a>". US Patent Number 20200125975, 2020.</li>
</ul> 
* * *

<p align="center"><a href="https://medium.com/@pezeshkp">Medium</a>    &nbsp;&nbsp;&nbsp;   <a href="https://medium.com/series/my-writings-4de58ce3f034">My Writings</a>  &nbsp;&nbsp;&nbsp;   <a href="https://www.amazon.com/dp/B08DMK9PNM/ref=sr_1_1?dchild=1&keywords=a+bypass+to+nirvana&qid=1595806371&sr=8-1">My Book</a> </p>

